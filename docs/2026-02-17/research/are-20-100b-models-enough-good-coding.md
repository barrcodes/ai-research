# Are 20-100B Models Enough for Good Coding?

| | |
|---|---|
| **Source** | Reddit - LocalLLaMA |
| **URL** | [old.reddit.com/r/LocalLLaMA/comments/1r6g4s9](https://old.reddit.com/r/LocalLLaMA/comments/1r6g4s9/are_20100b_models_enough_for_good_coding/) |
| **Researched** | 2026-02-17 |
| **Status** | Post unavailable - deleted or removed |

## Overview

The target Reddit discussion is no longer accessible. The post has been deleted or removed from the LocalLLaMA subreddit, returning a standard "page not found" error from both old.reddit.com and www.reddit.com.

## Attempted Access

- Original URL: `https://old.reddit.com/r/LocalLLaMA/comments/1r6g4s9/are_20100b_models_enough_for_good_coding/`
- Both old.reddit.com and www.reddit.com variants return 404/not found errors
- Attempted access date: 2026-02-17

## Recommendation

To access community consensus on this topic, consider:
1. Searching the LocalLLaMA subreddit for recent similar discussions about model size vs. coding quality
2. Checking r/LocalLLaMA top posts from recent weeks on the topic of model sizing
3. Exploring LLaMA-specific discussions on related topics (e.g., "which model size for coding", "Llama 70B vs smaller models")

## Sources

- [Reddit - LocalLLaMA](https://old.reddit.com/r/LocalLLaMA/) - Community discussion forum for local LLM deployment and use
