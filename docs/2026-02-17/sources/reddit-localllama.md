# Reddit r/LocalLLaMA

| | |
|---|---|
| **URL** | [old.reddit.com/r/LocalLLaMA](https://old.reddit.com/r/LocalLLaMA/top/?t=day) |
| **Scanned** | 2026-02-17 |
| **Since** | 2026-02-15 |
| **Articles** | 14 |
| **Deduplicated** | true |
| **Removed** | 0 |

---

## 4 of the top 5 most used models on OpenRouter this week are Open Source!

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6g8c2/4_of_the_top_5_most_used_models_on_openrouter/
- **Published:** 2026-02-16 (21 hours ago)
- **Description:** Discussion about open source models dominating OpenRouter's usage rankings, showing the competitive landscape of available LLMs.
- **Relevance:** Directly relevant to LLM adoption and performance in the wild; shows which open source models are gaining market traction.

## Difference Between QWEN 3 Max-Thinking and QWEN 3.5 on a Spatial Reasoning Benchmark (MineBench)

- **URL:** https://www.reddit.com/gallery/1r6h3ha
- **Published:** 2026-02-16 (20 hours ago)
- **Description:** Comparative analysis of newer Qwen model versions on spatial reasoning tasks, demonstrating capability improvements.
- **Relevance:** Benchmark analysis of LLM capabilities and model comparison between major releases.

## Fine-tuned FunctionGemma 270M for multi-turn tool calling - went from 10-39% to 90-97% accuracy

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6gvqz/finetuned_functiongemma_270m_for_multiturn_tool/
- **Published:** 2026-02-16 (20 hours ago)
- **Description:** Tutorial demonstrating significant improvement in tool calling accuracy for a small local model through fine-tuning, achieving 90-97% accuracy.
- **Relevance:** Highly relevant to tool use, agentic patterns, and making smaller models capable of function calling; practical guide for local model deployment.

## Are 20-100B models enough for Good Coding?

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6g4s9/are_20100b_models_enough_for_good_coding/
- **Published:** 2026-02-16 (19 hours ago)
- **Description:** Discussion exploring the capability boundaries of medium-sized models for coding tasks and code generation.
- **Relevance:** Directly relevant to coding assistants and LLM capabilities for programming tasks; practical evaluation of model sizes.

## Qwen3.5-397B up to 1 million context length

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6hqus/qwen3597b_up_to_1_million_context_length/
- **Published:** 2026-02-16 (14 hours ago)
- **Description:** Discussion about the extended context window capabilities of Qwen 3.5 large variant, enabling processing of extremely long documents.
- **Relevance:** Relevant to LLM capabilities and model releases; extended context is important for agentic systems processing long sequences.

## Google Deepmind has released their take on multi-agent orchestration they're calling Intelligent AI Delegation

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6h0yz/google_deepmind_has_released_their_take_on/
- **Published:** 2026-02-16 (15 hours ago)
- **Description:** Announcement and discussion of Google DeepMind's new multi-agent orchestration system for coordinating multiple AI systems.
- **Relevance:** Directly relevant to agentic patterns and multi-agent systems; demonstrates enterprise approaches to agent coordination.

## [Solution Found] Qwen3-Next 80B MoE running at 39 t/s on RTX 5070 Ti + 5060 Ti (32GB VRAM)

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6iv0v/solution_found_qwen3next_80b_moe_running_at_39_ts/
- **Published:** 2026-02-17 (5 hours ago)
- **Description:** Technical solution for running a large Mixture-of-Experts model locally with impressive throughput on consumer hardware.
- **Relevance:** Relevant to local LLM infrastructure and deployment; demonstrates practical optimization for high-performance local inference.

## Qwen 3.5, replacement to Llama 4 Scout?

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6iupo/qwen_35_replacement_to_llama_4_scout/
- **Published:** 2026-02-17 (5 hours ago)
- **Description:** Discussion comparing Qwen 3.5 against Llama 4 Scout models, evaluating which is the better choice for different use cases.
- **Relevance:** Relevant to LLM comparison and model selection; helps understand competitive positioning of major open source models.

## Tiny Aya

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6jf9b/tiny_aya/
- **Published:** 2026-02-17 (6 hours ago)
- **Description:** Announcement or discussion of the Tiny Aya model release, a smaller multilingual model.
- **Relevance:** Relevant to new LLM releases and model availability; smaller models are valuable for local deployment and resource-constrained environments.

## Local running Qwen3:14b helped fix my internet on Linux while offline

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6f8oi/local_running_qwen314b_helped_fix_my_internet_on/
- **Published:** 2026-02-16 (22 hours ago)
- **Description:** Real-world use case where a local Qwen model was used to help troubleshoot and fix network issues without internet access.
- **Relevance:** Demonstrates practical application of local LLMs for troubleshooting and technical problem-solving, showing utility of accessible AI tools.

## Qwen 3.5 goes bankrupt on Vending-Bench 2

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6g8k0/qwen_35_goes_bankrupt_on_vendingbench_2/
- **Published:** 2026-02-16 (21 hours ago)
- **Description:** Humorous/critical analysis showing Qwen 3.5 performance issues on the Vending-Bench 2 benchmark, suggesting capability limitations.
- **Relevance:** Relevant to LLM benchmarking and capability analysis; provides critical perspective on model limitations.

## Google doesn't love us anymore

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6gbf8/google_doesnt_love_us_anymore/
- **Published:** 2026-02-16 (21 hours ago)
- **Description:** Discussion about Google's shifting position on open source AI and their competitive relationship with local LLM community.
- **Relevance:** Industry dynamics affecting local LLM development and Google's role in the competitive landscape.

## Where are Qwen 3.5 2B, 9B, and 35B-A3B

- **URL:** https://old.reddit.com/r/LocalLLaMA/comments/1r6jh45/where_are_qwen_35_2b_9b_and_35ba3b/
- **Published:** 2026-02-17 (10 hours ago)
- **Description:** Community question about the availability of specific smaller Qwen 3.5 model variants and their release status.
- **Relevance:** Relevant to model releases and availability of smaller model sizes for local deployment.

## smol-IQ2_XS 113.41 GiB (2.46 BPW)

- **URL:** https://huggingface.co/ubergarm/Qwen3.5-397B-A17B-GGUF
- **Published:** 2026-02-17 (12 hours ago)
- **Description:** Resource listing for quantized GGUF version of Qwen 3.5 model, providing efficient local deployment option.
- **Relevance:** Relevant to LLM infrastructure and quantized model availability for local inference optimization.
