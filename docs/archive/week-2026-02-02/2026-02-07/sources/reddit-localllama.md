# Reddit r/LocalLLaMA

| | |
|---|---|
| **URL** | [old.reddit.com/r/LocalLLaMA](https://old.reddit.com/r/LocalLLaMA/) |
| **Scanned** | 2026-02-07 |
| **Since** | 2026-01-31 |
| **Articles** | 14 |

---

## I trained a 1.8M params model from scratch on a total of ~40M tokens.

- **URL:** https://www.reddit.com/gallery/1qym566
- **Published:** 2026-02-07
- **Description:** User shares experience training a very small language model from scratch with minimal computational resources. Relevant to local LLM development and training methodologies.
- **Relevance:** Directly relevant to local LLM training and optimization techniques for resource-constrained environments.

## Prompt injection is killing our self-hosted LLM deployment

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-07
- **Description:** Discussion about prompt injection vulnerabilities in self-hosted LLM deployments and strategies to mitigate them.
- **Relevance:** Relevant to LLM security and agentic patterns when using local models in production environments.

## I tested 11 small LLMs on tool-calling judgment â€” on CPU, no GPU.

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-07
- **Description:** Comparative testing of small language models for tool-calling capabilities on CPU-only hardware.
- **Relevance:** Highly relevant to agentic patterns, tool use, and LLM capabilities on local hardware without GPU requirements.

## Potential new Qwen and ByteDance Seed models are being tested on the Arena. The "Karp-001" and "Karp-002" models claim to be Qwen-3.5 models. The "Pisces-llm-0206a" and "Pisces-llm-0206b" models claim to be ByteDance models.

- **URL:** https://i.redd.it/rtrygqo1p2ig1.jpeg
- **Published:** 2026-02-07
- **Description:** Discussion about new unreleased or early-stage Qwen and ByteDance LLM models being tested in benchmarking arenas.
- **Relevance:** Relevant to new LLM model developments and benchmarking of frontier models.

## AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test.

- **URL:** https://i.redd.it/7euavxiwo4ig1.png
- **Published:** 2026-02-07
- **Description:** Benchmarking results showing LLM performance on AIME 2026 with cost analysis for DeepSeek V3.2.
- **Relevance:** Relevant to LLM benchmarks, capabilities assessment, and cost-efficiency of local/open models.

## Full Claude Opus 4.6 System Prompt for your pleasure

- **URL:** https://github.com/asgeirtj/system_prompts_leaks/blob/main/Anthropic/claude-opus-4.6.md
- **Published:** 2026-02-07
- **Description:** Leaked system prompt for Claude Opus 4.6 model shared on GitHub.
- **Relevance:** Highly relevant to Claude model documentation and understanding of frontier LLM capabilities.

## Successfully built an Autonomous Research Agent to handle 10k PDFs locally (32GB RAM / AnythingLLM)

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-07
- **Description:** Tutorial/guide on building a local autonomous research agent using AnythingLLM for document processing.
- **Relevance:** Highly relevant to agentic patterns and AI tools for local LLM deployments.

## Nemo 30B is insane. 1M+ token CTX on one 3090

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-06
- **Description:** Discussion about Nemo 30B model's impressive context window capabilities on single GPU hardware.
- **Relevance:** Relevant to local LLM capabilities and performance benchmarking on consumer hardware.

## Kimi-Linear-48B-A3B & Step3.5-Flash are ready - llama.cpp

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-06
- **Description:** News about new model releases (Kimi-Linear-48B-A3B and Step3.5-Flash) being ready for use with llama.cpp inference.
- **Relevance:** Relevant to new local LLM models and inference optimization tools.

## DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison.

- **URL:** https://www.reddit.com/gallery/1qycn5s
- **Published:** 2026-02-07
- **Description:** Tutorial comparing open-source LLMs on minimal hardware with OpenVINO optimization.
- **Relevance:** Relevant to running local LLMs on resource-constrained hardware and model optimization techniques.

## The M5 max and possibly the m5 ultra macs are coming soon!

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-07
- **Description:** Discussion about upcoming Apple M5 Mac hardware and implications for running local LLMs.
- **Relevance:** Relevant to hardware deployment considerations for local LLM inference.

## DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)

- **URL:** https://v.redd.it/lhz2yavkm2ig1
- **Published:** 2026-02-07
- **Description:** Demonstration of a portable operating system image with integrated LLM and knowledge base running from USB.
- **Relevance:** Relevant to portable local LLM deployment and self-hosted AI infrastructure.

## Built comprehensive Grafana monitoring for my LLM home server

- **URL:** https://www.reddit.com/gallery/1qyhppc
- **Published:** 2026-02-07
- **Description:** Guide on setting up monitoring infrastructure for locally-hosted LLM servers using Grafana.
- **Relevance:** Relevant to MLOps and deployment monitoring for local LLM infrastructure.

## Gemini System Prompt - Google decided to remove "PRO" option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription.

- **URL:** https://old.reddit.com/r/LocalLLaMA/
- **Published:** 2026-02-07
- **Description:** Shared system prompt extraction from Google's Gemini model.
- **Relevance:** Relevant to understanding frontier LLM system prompts and model behaviors.

## Benchmarking total wait time instead of pp/tg

- **URL:** https://i.redd.it/dmf3ykavv3ig1.png
- **Published:** 2026-02-07
- **Description:** Resource discussing alternative benchmarking metrics for LLM inference performance.
- **Relevance:** Relevant to LLM benchmarking and performance evaluation methodologies.
