# Reddit r/singularity

| | |
|---|---|
| **URL** | [old.reddit.com/r/singularity](https://old.reddit.com/r/singularity/top/?t=day) |
| **Scanned** | 2026-02-24 |
| **Since** | 2026-02-23 |
| **Articles** | 4 |

---

## IBM Stock Tumbles After Anthropic Launches Claude Code for COBOL Modernization

- **URL:** https://www.investing.com/news/stock-market-news/ibm-stock-tumbles-10-after-anthropic-launches-cobol-ai-tool-4519649
- **Published:** 2026-02-23
- **Description:** IBM stock plunged 10% following Anthropic's launch of a Claude Code tool designed to modernize COBOL legacy code. COBOL, a 66-year-old programming language, still processes approximately 95% of ATM transactions in the United States.
- **Relevance:** Direct coverage of Claude's new coding capability and AI's impact on legacy systems modernization.

## xAI and Pentagon Reach Deal to Use Grok in Classified Systems; Anthropic Given Ultimatum

- **URL:** https://old.reddit.com/r/singularity/comments/1jxb5gv/xai_and_pentagon_reach_deal_to_use_grok_in/
- **Published:** 2026-02-23
- **Description:** xAI has reached an agreement with the Pentagon to use Grok in classified systems, while Anthropic faces an ultimatum regarding its AI tools.
- **Relevance:** Major development in AI adoption by government agencies and competitive landscape between AI companies.

## OpenAI: At Least 16.4% of SWE Bench Verified Have Flawed Test Cases

- **URL:** https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified/
- **Published:** 2026-02-23
- **Description:** OpenAI reports that a significant portion of the SWE Bench Verified benchmark contains flawed test cases, impacting the reliability of software engineering AI evaluations.
- **Relevance:** Important research on AI code generation benchmarks and evaluation methodology for coding assistants.

## New Benchmark "InsanityBench": Gemini 3.1 Pro Scores 15%

- **URL:** https://i.redd.it/ysaqkbc7yelg1.png
- **Published:** 2026-02-24
- **Description:** A new benchmark called "InsanityBench" shows Gemini 3.1 Pro achieving a 15% score, indicating challenging evaluation metrics for cutting-edge LLMs.
- **Relevance:** LLM benchmark and capability evaluation for state-of-the-art models.
