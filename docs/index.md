# AI Research Articles

## Current Week

### [2026-02-11](./2026-02-11/index.md)

Agentic middleware consolidates: Entire ($60M seed) and OpenAI Frontier both address agent fleet management rather than point solutions, while Apple Xcode integrates Claude Agent SDK natively via MCP. GPT-5.3-Codex and Claude Opus 4.6 lead frontier coding benchmarks with distinct architectures—Codex emphasizes interactive steering during execution while Opus offers graduated adaptive reasoning. GLM-5 (745B MoE on Huawei Ascend) and MiniMax M2.1 ($19/month Agent Desktop) demonstrate cost/sovereignty alternatives. Security incidents dominate: OpenClaw's 42K exposed instances with 341 malicious skills prompted AgentVault emergency response; prompt injection enables API key theft via attacker-supplied credentials. MCP token optimization patterns mature—dynamic tool discovery achieves 75-95% reduction from static initialization. Evaluation rigor challenged: agentic eval variance of 2.2-6.0 percentage points invalidates most claimed 2-3 point improvements. SAE steering fails catastrophically for structured output (86.8% → 24.4% valid JSON), validating constrained decoding for syntax control. 11 sources, 32 articles researched.

### [2026-02-09](./2026-02-09/index.md)

Frontier models solidify agentic orchestration: Claude Opus 4.6's agent teams enable parallel multi-agent workflows natively, while SAE steering limitations clarify that structured output requires constrained decoding—not semantic control. AIRS-Bench emerges as the first genuine autonomy benchmark, measuring end-to-end research lifecycle capability (4/20 tasks solved across frontier models). Security surfaces proliferate: prompt injection in messaging apps exploits automatic URL preview to exfiltrate agent data; enterprise agent deployments show 58% visibility but only 37% actual control—a critical governance-containment gap. Terminology crisis around "async agents" exposes definitional fragmentation across orchestration models. Sparse MoE models (Qwen3, Step-3.5-Flash, GLM-5, LLaDA2.0) compress toward frontier capability with 6-10x efficiency gains via active-parameter optimization. Video synthesis quality reaches 95-98% accuracy (MOS 3.5-4.2) but full-deepfake detectability remains, limiting high-assurance applications. Medical AI benchmarks decouple from real-world performance (94% exam scores → 33% diagnostic accuracy), highlighting verification bottleneck. Local LLM deployment economics improve with linear attention (Kimi-Linear-48B) achieving 6x throughput on million-token contexts. 10 sources, 44 articles identified, 28 researched.

---

## Archived Weeks

- [Week of 2026-02-02](./archive/week-2026-02-02/week-2026-02-02.md)
- [Week of 2026-01-26](./archive/week-2026-01-26/week-2026-01-26.md)
