# Qwen3.5-122B-A10B Model Release

| | |
|---|---|
| **Source** | Hugging Face Model Card |
| **URL** | [huggingface.co/Qwen/Qwen3.5-122B-A10B](https://huggingface.co/Qwen/Qwen3.5-122B-A10B) |
| **Researched** | 2026-02-27 |

## Overview

Qwen3.5-122B-A10B is a sparse mixture-of-experts multimodal language model with 122B total parameters but only 10B activated per token. It combines gated attention and linear attention mechanisms (DeltaNet) across 16 interleaved layers, achieving strong performance on reasoning (SWE-bench 72.0), vision-language (OmniDocBench 89.8), and code tasks with native 262K token context extensible to 1M tokens.

## Key Points

- **Sparse Activation**: 256 total experts with 8 routed + 1 shared per token creates 8.2% parameter efficiency compared to dense models
- **Hybrid Attention Architecture**: Combines 16 layers of DeltaNet (linear attention, 64V/16QK heads) with Gated Attention (32Q/2KV heads), balancing speed and precision
- **Multimodal Capabilities**: Handles text, images, and video with unified embedding space; 201 language support
- **Production-Ready Inference**: vLLM integration with tensor parallelism, tool calling, reasoning parser, OpenAI-compatible API
- **Extended Context**: 262K tokens native; YaRN scaling enables 1M+ without retraining

## Technical Details

**Model Architecture**

| Component | Specification |
|-----------|---------------|
| Total Parameters | 122B |
| Activated per Token | 10B (8.2% efficiency) |
| Experts | 256 total (8 routed + 1 shared) |
| Hidden Dimension | 3,072 |
| Layers | 48 (16 DeltaNet-MoE, 16 Attention-MoE, 16 standard) |
| Context Length | 262K native → 1M+ with YaRN |

**Benchmark Performance**

| Task | Score | Percentile |
|------|-------|-----------|
| MMLU-Pro | 86.7 | Top-tier reasoning |
| SuperGPQA | 67.1 | Expert knowledge |
| SWE-bench Verified | 72.0 | Code generation |
| OmniDocBench | 89.8 | Document understanding |
| MathVision | 86.2 | Technical problems |

**Inference Configuration**

Deploy with 8× H100/A100 GPUs using vLLM's tensor parallelism (--tensor-parallel-size 8). Reasoning mode (temperature 1.0, top_p 0.95) handles complex tasks; instruct mode (temperature 0.7, top_p 0.8) for deterministic outputs. Minimum output 32K tokens; 81.9K recommended for complex problems. Disable thinking mode via `enable_thinking: False` in chat_template_kwargs for direct responses.

## Implications

**When to Deploy**
- Long-context document processing (contracts, codebases, research papers)
- Multimodal reasoning over technical documents and diagrams
- Code generation and debugging with SWE-bench-level performance
- Agents requiring tool calling and extended reasoning

**Hardware Trade-offs**
- 8-GPU cluster required for full 262K context; smaller deployments feasible but with context truncation
- Sparse activation (10B active) reduces memory vs 122B dense, but still needs 8× high-end GPUs
- Inference latency dominated by context length and output token requirements (32K minimum)

**Against Dense Alternatives**
- Sparse MoE introduces routing overhead; dense models may be faster for short sequences
- 122B parameter count still substantial despite 10B activation; consider Claude, GPT-4 API if GPU ownership not feasible
- Reasoning mode adds 50-100% latency; requires explicit `enable_thinking: False` for low-latency applications

**Architectural Learnings**
- Hybrid DeltaNet + attention interleaving outperforms single-mechanism designs for this scale
- MoE at 8.2% activation efficiency suggests diminishing returns on expert count beyond 256 experts
- Extended context viability without retraining (YaRN) opens budget-constrained long-context applications

## Sources
- [Qwen3.5-122B-A10B Model Card](https://huggingface.co/Qwen/Qwen3.5-122B-A10B) - Official specifications, benchmarks, and inference examples
