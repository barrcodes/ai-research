# Statement from Dario Amodei on Our Discussions with the Department of War

| | |
|---|---|
| **Source** | Anthropic |
| **URL** | [anthropic.com/news/statement-department-of-war](https://www.anthropic.com/news/statement-department-of-war) |
| **Researched** | 2026-02-27 |

## Overview

Anthropic's CEO Dario Amodei publicly outlined the company's response to Department of War pressure to remove safeguards from Claude's deployment in defense contexts. The statement rejects two military applications despite government threats: domestic mass surveillance and fully autonomous weapons systems. Amodei frames this as defending democratic governance while maintaining commitment to legitimate national security uses.

## Key Points

- **Rejected Mass Domestic Surveillance**: Anthropic refuses to enable AI-powered domestic surveillance despite government pressure, citing democratic principles and the capability of frontier AI to "assemble scattered data into a comprehensive picture of any person's life—automatically and at massive scale."

- **Rejected Autonomous Weapons**: Claude will not power weapons that remove human decision-making from targeting, based on reliability concerns ("frontier AI are simply not reliable enough") and risk to American personnel and civilians.

- **Government Coercion Tactics**: Department of War threatened to remove Anthropic from defense contracts and designate it a "supply chain risk"—a designation previously reserved for hostile nations—for refusing to remove safeguards.

- **Inherent Contradiction**: Amodei highlights the logical inconsistency of simultaneously treating Claude as a critical national security tool and a security threat requiring isolation.

## Technical Details

The statement identifies two core technical reliability constraints:
1. **Surveillance at scale**: Modern frontier models cannot be trusted with comprehensive personal data assembly across populations
2. **Autonomous targeting**: Current AI systems lack the reliability margin required for weapons without human verification in the decision loop

Amodei's framing suggests Anthropic has deployed Claude to defense agencies for legitimate purposes but with explicit restrictions on these high-risk applications.

## Implications

**For AI Governance**: This statement signals that major AI labs can resist government pressure while maintaining defense relationships—establishing precedent that safeguards are negotiable positions rather than immovable constraints. The threat of supply-chain designation shows governments treating AI as critical infrastructure with leverage over companies.

**For Military AI Development**: The rejection of autonomous weapons normalizes human-in-the-loop requirements. However, the willingness to serve defense applications broadly indicates the line is drawn at specific threat models, not defense use generally.

**For Democratic Institutions**: Domestic surveillance represents a different threat vector than foreign military AI. Amodei's public position treats these as governance failures rather than purely technical problems, framing the decision as defensive against authoritarianism.

**Practical Trade-offs**: Companies face pressure between government contracts and self-imposed governance standards. Anthropic's strategy—public refusal with transition assistance offers—suggests a middle path: maintain contracts while signaling non-negotiable boundaries.

## Sources

- [Anthropic Statement on Department of War Discussions](https://www.anthropic.com/news/statement-department-of-war) - Official position from Dario Amodei on safeguard boundaries and government pressure tactics