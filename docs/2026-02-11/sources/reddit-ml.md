# Reddit r/MachineLearning

| | |
|---|---|
| **URL** | [www.reddit.com/r/MachineLearning](https://www.reddit.com/r/MachineLearning/hot/) |
| **Scanned** | 2026-02-11 |
| **Since** | 2026-02-04 |
| **Articles** | 6 |

---

## [R] The Post-Transformer Era: State Space Models, Mamba, and What Comes After Attention
- **URL:** https://www.reddit.com/r/MachineLearning/comments/1r19jnu/r_the_posttransformer_era_state_space_models/
- **Published:** 2026-02-10 (21 hr. ago)
- **Description:** A practitioner's guide to Mamba and State Space Models exploring selective state spaces, linear scaling, and when to use SSMs vs Transformers vs hybrids. Includes discussion of production-ready models and a blog post analyzing these architectural innovations.
- **Relevance:** Directly relevant to LLM architecture innovations and transformer alternatives that are reshaping modern language model design.

## [R] I am looking for good research papers on compute optimization during model training, ways to reduce FLOPs, memory usage, and training time without hurting convergence
- **URL:** https://www.reddit.com/r/MachineLearning/comments/1r1pr3c/r_i_am_looking_for_good_research_papers_on/
- **Published:** 2026-02-11 (9 hr. ago)
- **Description:** A research thread requesting papers on compute optimization techniques including mixed precision, gradient checkpointing, optimizer efficiency, sparsity, distributed training (ZeRO, tensor/pipeline parallelism), and compute-optimal scaling laws like Chinchilla-style approaches.
- **Relevance:** Relevant to AI infrastructure and training efficiency, key concerns for scaling and deploying large language models.

## [R] I probed 6 open-weight LLMs (7B-9B) for "personality" using hidden states â€” instruct fine-tuning is associated with measurable behavioral constraints
- **URL:** https://www.reddit.com/r/MachineLearning/comments/1r1w34h/r_i_probed_6_openweight_llms_7b9b_for_personality/
- **Published:** 2026-02-11 (3 hr. ago)
- **Description:** Research investigating personality traits in open-weight LLMs by probing hidden states, demonstrating how instruction fine-tuning imposes behavioral constraints on model outputs.
- **Relevance:** Directly relevant to understanding LLM behavior and the effects of fine-tuning on model characteristics, important for coding assistants and AI tool development.

## [R] LLaDA2.1 vs Qwen3 30B A3B: Benchmarking discrete diffusion LLMs against autoregressive MoE models
- **URL:** https://www.reddit.com/r/MachineLearning/comments/1r1694q/r_llada21_vs_qwen3_30b_a3b_benchmarking_discrete/
- **Published:** 2026-02-10 (22 hr. ago)
- **Description:** Benchmarking study comparing discrete diffusion LLMs (LLaDA2.1) against autoregressive mixture-of-experts models (Qwen3 30B A3B), evaluating performance across different model architectures.
- **Relevance:** Relevant to LLM development, model architectures, and benchmarking practices for evaluating different approaches to language modeling.

## [R] On Randomness in Agentic Evals
- **URL:** https://www.reddit.com/r/MachineLearning/comments/1r0wpn8/r_on_randomness_in_agentic_evals/
- **Published:** 2026-02-10 (1 day ago)
- **Description:** Research examining the role of randomness in evaluating agentic systems, addressing methodological concerns in how agent performance is measured and benchmarked.
- **Relevance:** Directly relevant to agentic patterns and the evaluation of autonomous agent systems, crucial for assessing AI tool reliability.

## [R] I accidentally built a dataloader 10x faster than PyTorch's and I'm still processing this
- **URL:** https://www.reddit.com/r/MachineLearning/comments/1r1t2ig/r_i_accidentally_built_a_dataloader_10x_faster/
- **Published:** 2026-02-11 (6 hr. ago)
- **Description:** A project post discussing an optimized dataloader implementation that achieves 10x speedup compared to PyTorch's standard dataloader, with technical insights on the optimization approach.
- **Relevance:** Relevant to AI infrastructure and MLOps, focusing on performance optimization for model training pipelines.
